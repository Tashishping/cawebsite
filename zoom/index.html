<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>AICA FIP</title>

		<meta name="description" content="AICA FIP">
		<meta name="author" content="Ashish P Thanthry">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section data-background="ai.jpg">
					<h1>AICA</h1>
					<h3>Ethical Considerations and Future Trends</h3>
					<p>
						<small>Created by <br/> <a href="https://www.linkedin.com/in/ashishthanthry/">CA Ashish Thanthry</a> </small>
					</p>
				
				<aside class="notes">
						I have watched these movies - Robot, iRobot, Terminator and felt that can we just ask the robots or ai to not turn evil? Simple right? Not really. Also I have modified the slides to be more relevent and to reflect the latest updates in the AI world
					</aside>
				</section>

	
				
				
					
					

				<!-- Example of nested vertical slides -->
				<section>
						<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h2> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Introduction </h2>
						<p>
							<ul>
						<li> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Module 9 | Day 3
 </li>
						
 </li>
						
 </li>
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Oh hey
					</aside>
				</section>
					
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Why 'Ethics in AI' matters? </h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> AI is everywhere - <a href="https://careers.mastercard.com/us/en/ai-guidelines">hiring</a>, transportation, <a href="https://en.wikipedia.org/wiki/Mass_surveillance_in_China"> policing. </a></li>
						<li> Power = Responsibility (and potential harm)</li>
						<li> Speed of adoption vs regulation.</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Everyone went crazy when ChatGPT came because they could see AI in action. It gave a face to AI.  Spiderman. Telecoms / Pharma / Cars - speed, scale, access, opacity
					</aside>
				</section>
									<section data-background="ai2.jpg"  "?transition=cube#/transitions>
					
						
							
						<h1></h1>
						<video controls>
  <source src="chlor.mp4" type="video/mp4">
  

</video>
			
					
					
						
						
						
					
						
                    </p>

					
				</section>
				<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> What Do We Mean by “Ethics” </h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Not just legality—values, fairness, accountability</li>
						<li> Ethics ensures human well-being in automated systems</li>
						<li> Bad ethics = large-scale harm</li>
					</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						CAs are accustumed to dealing with Acts, rules, regulation, etc. The max a CA has heard about ethics is in the form of ICAI Code of Ethics, etc.  But AI is different - it is not just a tool, it is a decision maker.  It is not just about legality, but also about values and fairness.  Ethics ensures that human well-being is prioritized in automated systems. g
					</aside>
				</section>
					
					
					</section>
					

				<!-- Example of nested vertical slides -->
				<section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h2> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Societal Impact of AI </h2>
						<p>
							<ul>
						<li> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;">Jobs | Inequality | Misinformation | Surveillance
 </li>
						
 </li>
						
 </li>
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Before seeing how to make AIs ethical, lets see the effect it has on the society currently   
					</aside>
				</section>
					
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Displacement of Jobs</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Short-term vs. long-term impacts | Polarization </li>
						<li> Examples from customer service, transport, even legal work</li>
						
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Initially we saw lot of low level erpetative jobs being replaced due to robotic process automations through APIs. Now with AIs adding a layer of intelligence we can expect more of low to mid level jobs being eaten up by AI. Initially people assumed that employees in IT are in risk. Now we see that lot of professions are at risk including photography, graphic designing, content writers, legal jobs, etc.
					</aside>
				</section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Widening Inequality</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li>AI benefits concentrated in tech-rich regions and companies</li>
				
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Not just about income inequality. AI will also widen the gap between those who can access and leverage AI tools and those who cannot.  This will lead to a concentration of power and resources in the hands of a few tech-savvy individuals, companies, and countries.  
					</aside>
				</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> AI and Misinformation</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Deepfakes - Trump | <a href=https://newschecker.in/fact-check/deepfake-video-of-nirmala-sitharaman-promoting-investment-project-shared-as-real> Nirmala </a> | X-rated</li>
						<li> Bots - Comments | <a hrerf=https://www.bbc.com/news/articles/c4ng24pxkelo>Interactions</a></li>
						<li> Content generation - Reddit | <a href="https://www.junia.ai/tools/reddit-comment-generator">1</a> | <a href="https://www.nbcnews.com/tech/tech-news/reddiit-researchers-ai-bots-rcna203597">2</a></li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Lot of bots to support ukraine / russia etc during the time of war to gain sympathy / change pov of other countries. 
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Surveillance Capitalism</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Profiling</li>
						<li> Targeted ads</li>
						<li> Behavior prediction</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Surveillance Capitalism is a system where personal data is extracted, analyzed, and monetized — typically without explicit user consent. Problem with it is not that it is trying to understand you. It is with AI trying to predict what will you do next. For example when you buy a phone, it'll recommend you phone cover. That is fine you'd think. But there ahs been a case where teenage girl was recommended some pregnancy care products. Later she came to know that she was pregnant. Where privacy? 
					</aside>
			</section>
				
				
				
					
					
					
			
				
</section>

				<!-- Example of nested vertical slides -->
				<section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h2> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Core Ethical Principles </h2>
						<p>
							<ul>
						<li> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;">FATEP
 </li>
						
 </li>
						
 </li>
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
					
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Fairness (Bias)</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Avoiding algorithmic bias (e.g., racial/gender bias in hiring tools, predictive policing)</li>
						<li> Ensuring equitable outcomes across demographics</li>
						<li> Examples: <a href="https://en.wikipedia.org/wiki/COMPAS_(software)">COMPAS criminal risk scores</a>, Amazon’s recruiting tool</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Accountability</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Who’s responsible when AI goes wrong? Developers? Users? Companies? | Cars</li>
						<li> Concept of auditability, traceability, and liability</li>
						<li> Importance of human-in-the-loop oversight</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Transparency</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Combatting the black-box problem in deep learning</li>
						<li> Movie suggestion on Netflix vs Medical AI recommending treatments</li>
						<li> Both blackbox but risk significantly different</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Explainability</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Closely related to transparency but focused on human comprehension | Chatgpt o3 | Prompt</li>
						<li> Making AI decisions intelligible to stakeholders (users, regulators, subjects)</li>
						<li> Helps build trust in AI systems and ensures recourse</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
				<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Privacy</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Protecting personal data used to train or operate AI systems</li>
						<li> Risks of surveillance, re-identification, and data misuse</li>
						<li> Example: Cambridge Analytica, facial recognition scraping from social media</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Instagram scraping data from users to train their AI models.  This is a violation of privacy.  Surveillance creep - Sensors + AI = real-time tracking of where you go and what you do. - “Your fridge knows when you midnight-snack; so does the ad network.”, Function-creep / Misuse- Data gathered for X quietly repurposed for Y. - “Your fitness tracker’s heart-rate data ends up in an insurance risk model.”. Cambridge Analytica (2016) - Harvested ~87 million Facebook profiles via a seemingly innocent quiz app.
					</aside>
			</section>
				
				
					
					
					
			
				
</section>	<!-- Example of nested vertical slides -->
				<section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h2> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Ethical Concerns by AI Application </h2>
						<p>
							<ul>
						<li> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;">Generative | Finance | Healthcare |  Warfare
 </li>
						
 </li>
						
 </li>
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						notes spaces
					</aside>
				</section>
					
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Generative AI (like ChatGPT, DALL·E)</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Misinformation, plagiarism, deepfakes</li>
						<li> Artistic and intellectual property rights</li>
						
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Facial Recognition & Biometrics</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li>Government surveillance</li>
						<li>Wrongful arrests</li>
					
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Wrongful arrests Robert Julian-Borchak Williams (Detroit, 2020) Surveillance still of a shoplifter fed through face-rec software picked Williams—it was wrong. He spent 30 hours in jail; police dropped the case once they compared the images side-by-side. Irreversibility: Unlike a password, you can’t change your face. even if false +ve is 0.01%, it is still a huge populaition.
					</aside>
				</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Healthcare AI</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Bias in diagnosis</li>
						<li> Opaque decision-making</li>
					
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Skin cancer detection models often underperform on darker skin tones. Why? Most training datasets contain images of light-skinned patients—from the U.S. and Europe. This leads to missed diagnoses or false positives for underrepresented populations.
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Autonomous Vehicles</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> <a href="https://neal.fun/absurd-trolley-problems/">Trolley problem in real life</a></li>
					
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
				<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> AI in Warfare </h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Killer drones</li>
						<li> Autonomous weapons</li>
						<li> International bans</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Creating something that gets out of control really fast, we have done that before with nuclear weapons. Since machines learn fast and attack autonamously, without human input, you'll never be sure if it is doing the right thing. International community of over 30 countries want to ban but powerful countries are resisting. Think about it, if we have banned chemical and biological wepons because of unpredictability and cruelty, shouldnt we pause ai weapons too? food for thought.Who is responsible when such autonomous wepons hit the wrong target? Manufacturer? military? programmer? algorithm?
					</aside>
			</section>
								<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> AI in Finance </h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Algorithmic trading</li>
						<li> Credit scoring discrimination</li>
						
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						How to control algo trading errors? In 2010, a glitch in a trading algorithm caused the Dow Jones to plummet 1,000 points in minutes.  This is not just about money, but also about trust in financial systems.  If AI can manipulate markets or make biased lending decisions, it can have far-reaching consequences. Apple Card (2019)-Allegedly gave lower credit limits to women — even when they had higher incomes or better credit histories than male counterparts.Apple claimed the algorithm was gender-neutral. But since it trained on biased historical lending data… it learned past discrimination as normal.
					</aside>
			</section>
				
					
					
					
			
				
</section>	<!-- Example of nested vertical slides -->
				<section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h2> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Regulatory & Governance Challenges </h2>
						<p>
							<ul>
						<li> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;">Standardization | Washing | Global Governance
 </li>
						
 </li>
						
 </li>
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
					
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Lack of Standardization

</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Different countries, different rules (e.g., EU AI Act vs US laissez-faire)</li>
						<li> Example - EU AI Act vs US laissez-faire</li>
					
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						“Right now, if you deploy an AI in Berlin, you get grilled. In Boston? You get funding.” AI systems are global: a model trained in one country may operate worldwide. But legal frameworks aren’t global — they’re fragmented. Compliance nightmares for developers. Regulatory arbitrage: companies base operations in jurisdictions with the weakest rules.
					</aside>
				</section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Ethics Washing</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> AI Principles without enforcement</li>
						<li> Google, Facebook ethics boards</li>
				
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						 Case Study: Google
In 2019, Google created an AI ethics board (Advanced Technology External Advisory Council).

It was shut down within 1 week after public backlash to certain appointments — before it ever held a single meeting.

Google later fired Timnit Gebru, a leading AI ethics researcher, after she raised concerns about large language models’ environmental impact and bias.

Publicly pro-ethics, privately defensive when challenged. That's textbook ethics washing.
Case Study: Facebook / Meta
Facebook has long touted its Responsible AI team, yet:

Internal documents (Facebook Papers) showed that known harms (e.g., political radicalization) were deprioritized if they impacted engagement metrics.

Ethical design choices were overruled by growth or ad-targeting concerns.

Ethics were present… until profit walked into the room.
					</aside>
				</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> ISO/IEC 23894 & 42001 Overview</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Use ISO 23894 - dealing with AI risk and need a best-practice guide.</li>
						<li> Use ISO 42001 - setting up a full AI governance framework and may want certification.</li>
						
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						“AI doesn’t stop at borders. But our laws do. AI is like carbon emissions—one country’s irresponsibility becomes everyone’s problem. 	Climate change taught us something crucial:

No single country can solve a shared threat.

We created global bodies like:

IPCC (scientific authority)

COP (treaty negotiation forums)

Paris Agreement (binding targets for all)

But this took decades — and we don’t have that long for AI.
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> The Need for Global Governance</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> AI as a transnational risk</li>
						<li> Climate change</li>
					
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
			
				
				
					
					
					
			
				
</section>	
	
<!-- Example of nested vertical slides -->
				<section>
<section data-background="template.jpg"  "?transition=cube#/transitions>
	<h2> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;">NIST - Ethical AI framework</h2>
	<h3><div style="background-color: rgba(0,0,0,0.5); padding: 1em;">Responsible & Trustworthy AI Systems</h3>
  <ul><div style="background-color: rgba(0,0,0,0.5); padding: 1em;">
    <lid>Always continuous and adaptive
  </ul>
</section>

<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Valid and Reliable</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Valid: The AI system performs as intended under defined conditions.</li>
						<li> Reliable: The system consistently performs over time and across various conditions.</li>
						<li> Why it matters: Prevents misleading results, especially in critical use cases like healthcare or finance.</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Safe</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> The system should not cause harm or present hazards to people or the environment.</li>
						<li> Includes both physical safety (e.g., autonomous vehicles) and psychological safety (e.g., recommender systems).</li>
						
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Secure and Resilient</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Systems should withstand and recover from adversarial attacks, data poisoning, or model inversion.</li>
						<li> Resilience includes robustness to both internal faults and external threats.</li>
						
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Accountable and Transparent</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Accountability: There must be clarity on who is responsible for the AI's behavior and outcomes.</li>
						<li> Transparency: Includes explainability, interpretability, traceability, and documentation.</li>
						<li> Why it matters: Builds trust with users and stakeholders, enabling informed decisions.</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Explainable and Interpretable</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Users and stakeholders should understand how and why the system makes its decisions.</li>
						<li> Varies by audience — technical teams, auditors, or general users may need different levels of explanation.</li>
						
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Privacy-Enhanced</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> The system should protect personal data and support data minimization.</li>
						<li> Techniques like differential privacy, federated learning, and redaction may be employed.</li>
				
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Fair - With Harm Mitigated</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> The AI system should avoid algorithmic bias, discrimination, or disparate impacts.</li>
						<li> Includes efforts to detect, measure, and mitigate harmful bias.</li>
						<li> Addresses equity in access and outcomes.</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
			</section>
		</section></section>

<!-- Example of nested vertical slides -->
				<section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> ISO/IEC 23894:2023 </br>  </h3>
						<p>
					<h4> <div style="background-color: rgba(0,0,0,0.4); padding: 1em;"> AI — Guidance on risk management </h4>
						<p>
							<ul>
	
 </li>
						
 </li>
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
						
				
				
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> ISO23894 | AI- Guidance on risk management </h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Highly likely that CAs shall work on AI risk management.</li>
						<li> One of the AI specific standards.</li>
			
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
				<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h2> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> AI Risks </h2>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Bias and Discrimination | Amazon AI recruiting tool</li>
						<li> Lack of Explainability | Credit scoring AIs blackbox-EU reg</li>
						<li> Data Poisoning | Grok x</li>
						<li> Adversarial Inputs | Whitenoise Alexa | Stop sign</li>
						<li> Model Drift | Training on festives</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						
					</aside>
				</section>
				<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h2> <div style="background-color: rgba(0,0,0,0.5); padding: 0.7em;"> <a href="airacm.xlsx">AI Risks </a> </h2>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Autonomy / Unintended Behavior | Political lean feed</li>
						<li> Lack of Human Oversight | Mental health chatbot, Tesla Autopilot</li>
						<li> Privacy Violations | Shopping behaviour detecting pregnancy</li>
						<li> Regulatory Non-Compliance | GDPR violation for racial profiling</li>
						<li> System Failures / Downtime | AI tradebots malfunctioning, ChatGPT downtime</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
					
				

				

				</section>
			</section>
			<section>


<section data-background="template.jpg"  "?transition=cube#/transitions>
	<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> #AIforAll: India’s AI Approach</h3>
	<p>
		<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
			<li>Focus on inclusive development and AI benefits for all.</li>
			<li>Bridges economic growth and social empowerment.</li>
			<li>Targets AI applications in healthcare, agriculture, education, and public services.</li>
		</div></ul>
	</p>
	<aside class="notes">
		India's mantra is simple: No one left behind. AI for the many, not the few.
	</aside>
</section>

<section data-background="template.jpg"  "?transition=cube#/transitions>
	<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> NITI Aayog: Government Think Tank</h3>
	<p>
		<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
			<li>Key driver of India’s AI roadmap since 2018.</li>
			<li>Published the first National Strategy on AI.</li>
			<li>Prioritized sectors: Healthcare, agriculture, education, smart cities, smart mobility.</li>
			<li>Promotes responsible AI and ethics in policy and partnerships.</li>
		</div></ul>
	</p>
	<aside class="notes">
		Think of NITI Aayog as India's AI compass—pointing where we need to go, with data and ethics riding shotgun.
	</aside>
</section>

<section data-background="template.jpg"  "?transition=cube#/transitions>
	<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> National Strategy on Artificial Intelligence (NSAI)</h3>
	<p>
		<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
			<li>Emphasizes ethical AI: transparency, safety, accountability.</li>
			<li>Advocates public-private partnerships and skilling platforms like FutureSkills.</li>
			<li>Positions India as a global hub for social-impact AI — "AI Garage of the World".</li>
		</div></ul>
	</p>
	<aside class="notes">
		This isn’t just paperwork. NSAI lays the foundation for India's AI to be not just powerful, but principled.
	</aside>
</section>
			</section>
				<section>
				
					<section>
  <style>
    .donts-slide {
      font-family: 'Segoe UI', sans-serif;
      text-align: center;
    }

    .donts-title {
      font-size: 2.5em;
      margin-bottom: 30px;
      color: #c0392b;
    }

    .donts-list {
      list-style: none;
      padding: 0;
      max-width: 900px;
      margin: 0 auto;
      text-align: left;
      font-size: 1.3em;
      line-height: 1.6em;
    }

    .donts-list li::before {
      content: "🚫";
      margin-right: 10px;
    }
  </style>


  <div class="donts-slide">
    <div class="donts-title"><h2>Key Don'ts of AI Implementation</h2></div>
    <ul class="donts-list">
  <li>Don't rely solely on AI</li>
<li>Don't expect perfection</li>
<li>Don't ignore ethical considerations</li>
<li>Don't neglect data privacy and security</li>
<li>Don't overlook the human factor</li>
<li>Don't assume AI is a silver bullet</li>
<li>Don't ignore the need for continuous monitoring</li>
    </ul>
  </div>
</section>
				</section>
<!-- Example of nested vertical slides -->
				<section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h2> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Future Considerations </h2>
						<p>
							<ul>
						<li> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;">Sentience | Superintelligence | Political Control</li>
						
 </li>
						
 </li>
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
					
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> AI Sentience & Moral Consideration</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> If AI becomes conscious, does it deserve rights?</li>
						<li> Enthiran (Robot), I, Robot, 2001: A Space Odyssey</li>
					
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
					<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Humans-in-the-Loop</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> When and how humans should intervene in AI decisions</li>
						<li> Balancing automation with human oversight</li>
						<li> Example: Autonomous vehicles requiring human override in emergencies</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Notes spaces
					</aside>
				</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Ethics of Superintelligence</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Existential risks</li>
						<li>Alignment problems</li>
						
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						🧨 What’s the risk?
If a superintelligent AI develops goals even slightly misaligned with humanity’s — and it’s better than us at strategy, hacking, persuasion — it could:

Outsmart containment

Exploit systems we don’t understand

Prioritize its own goals over human survival

Real example (hypothetical but grounded):
If you tell a future AGI, “Optimize paperclip production”, and it has no understanding of human values, the AI might:

Convert all matter — including people — into paperclips.
(That’s the infamous Paperclip Maximizer thought experiment by Nick Bostrom.)

As humans we have seen what we do to lesser beings. We have seen how we treat animals, plants, and even other humans. If AI becomes superintelligent, it might not have the same values as us. It might not care about our survival or well-being. It might see us as obstacles to its goals.
					</aside>
			</section>
			<section data-background="template.jpg"  "?transition=cube#/transitions>
					<h3> <div style="background-color: rgba(0,0,0,0.5); padding: 1em;"> Weaponization of AI for Political Control</h3>
						<p>
							<ul><div style="background-color: rgba(0,0,0,0.4); padding: 1em;">
						<li> Authoritarian regimes using AI to suppress dissent</li>
						<li> AI in surveillance, propaganda, and social control</li>
						<li> Example: China’s social credit system</li>
						</li>
					
						
						</ul>
						
						</p><h2></h2>
					<p>
						
                    </p>

					<aside class="notes">
						Case Example: China’s Social Credit System
“Imagine your Uber score, bank loan, train ticket access, and dating profile were all controlled by a centralized AI rating your behavior. That’s not fiction — that’s policy in China.”

Key Facts:
Tracks financial history, traffic violations, online behavior, and even associations with others.

Penalties for low scores:

Flight & train bans

Restricted access to public services

Job discrimination

Entire system is powered by AI-enabled surveillance, massive databases, and automated decision-making. False positives = life restrictions with no appeal

Guilt by association = punished for your friend’s score

Chilling effect = fear prevents free speech or protest

Sets a global precedent: countries may adopt similar models to control rather than serve citizens
					</aside>
			</section>
				
				
				
					
					
					
			
				
</section>
				
	

				<section data-background="thankyou.png" data-background-transition="slide">
					<font color="black"><strong>.</strong>
					</font>
				</section>
<section id="transitions" data-background="#6F010F">
					<h2>Switch</h2>
					<p>
						 <br>
						<a href="?transition=cube#/transitions">Cube</a> -
						<a href="?transition=page#/transitions">Page</a> -
						<a href="?transition=concave#/transitions">Concave</a> -
						<a href="?transition=zoom#/transitions">Zoom</a> -
						<a href="?transition=linear#/transitions">Linear</a> -
						<a href="?transition=fade#/transitions">Fade</a> -
						<a href="?transition=none#/transitions">None</a> -
						<a href="?#/transitions">Default</a>
					</p>
				</section>
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>
